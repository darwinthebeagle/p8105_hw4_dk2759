---
title: "p8105_hw5_dk2759"
author: "Darwin Keung"
date: "11/7/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

This [zip file](http://p8105.com/data/hw5_data.zip) contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

##### Goal: Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

Extract the file paths and names

```{r extract}
filepath = list.files("./data/problem_1", pattern = "*.csv", full.names = TRUE)
filename = basename(filepath)
# pattern selects for only .csv file types
# basename removes all of the path up to and including the last path separator (if any).
```

Function to read the data

```{r function}
read = function(data, name){
  list(read_csv(file = data) %>% mutate(id = name))
}
```

Iterate with `map` to read all the csv files and bind them together

```{r map, message=FALSE}
path_name = purrr::map2(filepath, filename, read)
df = purrr::map_df(path_name, bind_rows)
```

Tidy Data from wide to long form, separate the study arms and IDs

```{r tidy}
tidy_df = df %>%
  gather(key = week, value = score, week_1:week_8) %>% 
  mutate(id = str_replace(id, ".csv",""), 
         week = as.numeric(str_replace(week, "week_", ""))) %>% 
  separate(id, into = c("arm", "id"), sep = "_") %>% 
  mutate(id = as.factor(id)) %>% 
  mutate(arm = str_replace(arm, "con", "control")) %>% 
  mutate(arm = str_replace(arm, "exp", "experimental"))
skimr::skim(tidy_df)
```

These longitudinal study data contains 160 observations and 4 variables (`arm` : character. `id` : factor. `score` and `week` : numeric). `arm` is either experimental or control. The study duration lasts 8 weeks, there are 10 subjects in each arm.

### Visualization 

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r plot}
ggplot(tidy_df, aes(x = week, y = score, color = id, group = id )) + 
  geom_point(alpha = 0.5) + 
  geom_line() + 
  facet_grid(~arm) +
labs(
    title = "Study Observations", 
    x = "Week", 
    y = "Observation", 
    color = "Subject")
```

In the experimental group the scores trend upwards over time while the control groups stay roughly constant. Don't have enough information to say what the numbers represent. 

# Problem 2

The _Washington Post_ has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository [here](https://github.com/washingtonpost/data-homicides). You can read their accompanying article [here](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/). 

#### Data import

```{r import}
homicide_df = read_csv("./data/problem_2/wapo_homicide_data.csv", col_names = TRUE)
```

Describe the raw data:

These data have `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. The variables include a unique id, report date, first and last names of victims, age, sex, race, location (city, state, lat, long), and disposition. 

#### Data manipulation

Create a city_state variable (e.g. “Baltimore, MD”) 

```{r city_state}
homicide_df = homicide_df = read_csv("./data/problem_2/wapo_homicide_data.csv", col_names = TRUE) %>% 
  mutate(city_state = paste(city, state, sep = ", "))
# sep = ", " is also the same as city, ",", " ", state)
```

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r summarize_homicides}
summ_homicide_df = homicide_df %>% 
  group_by(city_state) %>% 
  mutate(homicide_total = n()) %>% 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") %>%
  mutate(homicide_unsolved = n()) %>% 
  select(city_state, homicide_unsolved, homicide_total) %>%
  distinct(.keep_all = FALSE)

summ_homicide_df
```

Summary of homicides displays the data as `r nrow(summ_homicide_df)` observations of data, and 3 variables: city_state, homicide_unsolved, homicide_total.

#### Balitmore Homicides

_For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe._

```{r prop.test_Baltimore}
# Summary of Baltimore homicides
baltimore = summ_homicide_df %>% 
  filter(city_state == "Baltimore, MD")

# proportion testing, estimating number of unsolved
prop_unsolved_baltimore = prop.test(baltimore[[2]], baltimore[[3]]) %>% 
  broom::tidy() %>%  
  select(estimate, conf.low, conf.high) %>%
  knitr::kable(digits = 3)
```


#### Prop.test for all cities

_Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city._

```{r prop.test_all_cities}
# write function for prop_test for mapping later
prop_test = function(var_numerator, var_denominator) {
  
  try(prop.test(var_numerator, var_denominator) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high), silent = FALSE)
  
}
# try is a wrapper to run an expression that might fail and allow the user's code to handle error-recovery. silent hides the errors.

prop_test_unsolved_cities = 
  summ_homicide_df %>% 
  mutate(prop_test = map2(homicide_unsolved, homicide_total, prop_test)) %>%
  unnest() %>% 
  ungroup() %>% 
  janitor::clean_names()

prop_test_unsolved_cities
```

Comment on prop.test for all cities:

Wrote a function to run a `prop.test` mapped the estimates and confidence intervals to each homicide data point for each city. Added 3 new variables of estimates, and low and high confidence limits in a 95% confidence interval. 

#### Plotting the estimates and CIs

**Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides**

```{r plot_est_CI}
prop_test_unsolved_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + 
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size = 10)) +
  labs(
    title = "Proportion Unsolved Homicides for 50 Large American Cities from Last Decade", 
    x = "City",
    y = "Estimate of Proportion of Unsolved Homicides",
    caption = "Data source from Washington Post, error bars represent 95% confidence interval"
  )
```

Comment on plot: Richmond, VA has the lowest estimated unsolved homicides, whereas Chicago, IL has the most, over 70% with the narrowest confidence interval amongst the 50 cities! 
New York, NY is within the top 15 proportion of solved homicides with around 40%. 