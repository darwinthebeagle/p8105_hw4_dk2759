---
title: "p8105_hw5_dk2759"
author: "Darwin Keung"
date: "11/7/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

This [zip file](http://p8105.com/data/hw5_data.zip) contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

### Goal: Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

Extract the file paths and names

```{r extract}
filepath = list.files("./data/problem_1", pattern = "*.csv", full.names = TRUE)
filename = basename(filepath)
# pattern selects for only .csv file types
# basename removes all of the path up to and including the last path separator (if any).
```

Function to read the data

```{r function}
read = function(data, name){
  list(read_csv(file = data) %>% mutate(id = name))
}
```

Iterate with `map` to read all the csv files and bind them together

```{r map, include = FALSE}
path_name = purrr::map2(filepath, filename, read)
df = purrr::map_df(path_name, bind_rows)
```

Tidy Data from wide to long form, separate the study arms and IDs

```{r tidy}
tidy_df = df %>%
  gather(key = week, value = score, week_1:week_8) %>% 
  mutate(id = str_replace(id, ".csv",""), 
         week = as.numeric(str_replace(week, "week_", ""))) %>% 
  separate(id, into = c("arm", "id"), sep = "_") %>% 
  mutate(id = as.factor(id)) %>% 
  mutate(arm = str_replace(arm, "con", "control")) %>% 
  mutate(arm = str_replace(arm, "exp", "experimental"))
skimr::skim(tidy_df)
```

These longitudinal study data contains 160 observations and 4 variables (`arm` : character. `id` : factor. `score` and `week` : numeric). `arm` is either experimental or control. The study duration lasts 8 weeks, there are 10 subjects in each arm.

### Visualization 

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r plot}
ggplot(tidy_df, aes(x = week, y = score, color = id, group = id )) + 
  geom_point(alpha = 0.5) + 
  geom_line() + 
  facet_grid(~arm) +
labs(
    title = "Study Observations", 
    x = "Week", 
    y = "Observation", 
    color = "Subject")
```

In the experimental group the scores trend upwards over time while the control groups stay roughly constant. Don't have enough information to say what the numbers represent. 

# Problem 2

The _Washington Post_ has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository [here](https://github.com/washingtonpost/data-homicides). You can read their accompanying article [here](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/). 

#### Data import

```{r import}
homicide_df = read_csv("./data/problem_2/wapo_homicide_data.csv", col_names = TRUE)
```

Describe the raw data:

These data have `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. The variables include a unique id, report date, first and last names of victims, age, sex, race, location (city, state, lat, long), and disposition. 

#### Data manipulation

Create a city_state variable (e.g. “Baltimore, MD”) 

```{r city_state}
homicide_df = homicide_df = read_csv("./data/problem_2/wapo_homicide_data.csv", col_names = TRUE) %>% 
  mutate(city_state = paste(city, state, sep = ", "))
# sep = ", " is also the same as city, ",", " ", state)
```


Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r summarize_homicides}
homicide
```

Total number of homicides in Baltimore, MD

Number unsolved homicide in Baltimore, MD
```{r summarize_unsolved}

```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides


==============
Notes to self
tried this, some sort of directory error to get my function to work.

#```{r build_dataframe}
# list the files
df = tibble(list.files(path = "./data"))
is.list(df)

# make a function for reading out each csv in my list
list_df <- function(file_name) {
  read_csv(paste0("data", df))
}

# map
mutate(map(.x = df, .f = list_df))
```.
